{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyM1tpK1L5avxmn1snmeGNZ8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "19b7c00964be485cb1b0412de7ee9a6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_074f822c410e4f08b59531aea5f1c9d2",
              "IPY_MODEL_6b26a437e85649d98fff8e0cf802a40a",
              "IPY_MODEL_334fd68df6e044af9ba12c4906ac22d8"
            ],
            "layout": "IPY_MODEL_ed2b52119886472790022d3896f5bb91"
          }
        },
        "074f822c410e4f08b59531aea5f1c9d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc1462b58eef4baca8a478f3b42c4a5b",
            "placeholder": "​",
            "style": "IPY_MODEL_9785e67ee8504c08b6e67e24faa371c9",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "6b26a437e85649d98fff8e0cf802a40a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfcc55357c734f729fa58df938dcfaad",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dfec84c004504e60bf445065a7a56c1b",
            "value": 7
          }
        },
        "334fd68df6e044af9ba12c4906ac22d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ee82215b11d49fd9aab8942ee7eb76b",
            "placeholder": "​",
            "style": "IPY_MODEL_2c66c9f792c945de921e4c00b5ea2841",
            "value": " 7/7 [00:11&lt;00:00,  1.52s/it]"
          }
        },
        "ed2b52119886472790022d3896f5bb91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc1462b58eef4baca8a478f3b42c4a5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9785e67ee8504c08b6e67e24faa371c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cfcc55357c734f729fa58df938dcfaad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfec84c004504e60bf445065a7a56c1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ee82215b11d49fd9aab8942ee7eb76b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c66c9f792c945de921e4c00b5ea2841": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KevinWang676/ChatGLM2-Voice-Cloning/blob/main/ChatGLM2_VC_SadTalker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eBPmqhlq6C6",
        "outputId": "7116d792-3685-43de-b0a2-d632bc2a7ee2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ChatGLM2-SadTalker'...\n",
            "remote: Enumerating objects: 426, done.\u001b[K\n",
            "remote: Counting objects: 100% (426/426), done.\u001b[K\n",
            "remote: Compressing objects: 100% (420/420), done.\u001b[K\n",
            "remote: Total 426 (delta 51), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (426/426), 14.94 MiB | 16.16 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n",
            "Filtering content: 100% (10/10), 2.78 GiB | 43.06 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/spaces/kevinwang676/ChatGLM2-SadTalker.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ChatGLM2-SadTalker/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2PzWpC2sZQS",
        "outputId": "3ab1fd51-bc36-4f02-933a-14912451c8ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ChatGLM2-SadTalker\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kidt_9m1smEo",
        "outputId": "64f9e661-9ba6-4581-8e36-a3eecc64881c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.0.1+cu118)\n",
            "Collecting numpy==1.23.4 (from -r requirements.txt (line 2))\n",
            "  Downloading numpy-1.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting face_alignment==1.3.5 (from -r requirements.txt (line 3))\n",
            "  Downloading face_alignment-1.3.5-py2.py3-none-any.whl (29 kB)\n",
            "Collecting imageio==2.19.3 (from -r requirements.txt (line 4))\n",
            "  Downloading imageio-2.19.3-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting imageio-ffmpeg==0.4.7 (from -r requirements.txt (line 5))\n",
            "  Downloading imageio_ffmpeg-0.4.7-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting librosa==0.9.2 (from -r requirements.txt (line 6))\n",
            "  Downloading librosa-0.9.2-py3-none-any.whl (214 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (0.56.4)\n",
            "Collecting resampy==0.3.1 (from -r requirements.txt (line 8))\n",
            "  Downloading resampy-0.3.1-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydub==0.25.1 (from -r requirements.txt (line 9))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: scipy==1.10.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (1.10.1)\n",
            "Collecting kornia==0.6.8 (from -r requirements.txt (line 11))\n",
            "  Downloading kornia-0.6.8-py2.py3-none-any.whl (551 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m551.1/551.1 kB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (4.65.0)\n",
            "Collecting yacs==0.1.8 (from -r requirements.txt (line 13))\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (6.0)\n",
            "Collecting joblib==1.1.0 (from -r requirements.txt (line 15))\n",
            "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.0/307.0 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-image==0.19.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (0.19.3)\n",
            "Collecting basicsr==1.4.2 (from -r requirements.txt (line 17))\n",
            "  Downloading basicsr-1.4.2.tar.gz (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting facexlib==0.3.0 (from -r requirements.txt (line 18))\n",
            "  Downloading facexlib-0.3.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio (from -r requirements.txt (line 19))\n",
            "  Downloading gradio-3.37.0-py3-none-any.whl (19.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gfpgan (from -r requirements.txt (line 20))\n",
            "  Downloading gfpgan-1.3.8-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting av (from -r requirements.txt (line 21))\n",
            "  Downloading av-10.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.0/31.0 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors (from -r requirements.txt (line 22))\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers (from -r requirements.txt (line 23))\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m134.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting webrtcvad==2.0.10 (from -r requirements.txt (line 24))\n",
            "  Downloading webrtcvad-2.0.10.tar.gz (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 25)) (3.20.3)\n",
            "Collecting cpm_kernels (from -r requirements.txt (line 26))\n",
            "  Downloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mdtex2html (from -r requirements.txt (line 27))\n",
            "  Downloading mdtex2html-1.2.0-py3-none-any.whl (13 kB)\n",
            "Collecting sentencepiece (from -r requirements.txt (line 28))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate (from -r requirements.txt (line 29))\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting loguru (from -r requirements.txt (line 30))\n",
            "  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting edge_tts (from -r requirements.txt (line 31))\n",
            "  Downloading edge_tts-6.1.7-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: altair in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 32)) (4.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from face_alignment==1.3.5->-r requirements.txt (line 3)) (4.7.0.72)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio==2.19.3->-r requirements.txt (line 4)) (8.4.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 6)) (3.0.0)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 6)) (1.2.2)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 6)) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 6)) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 6)) (23.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.19.3->-r requirements.txt (line 16)) (3.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.19.3->-r requirements.txt (line 16)) (2023.7.10)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.19.3->-r requirements.txt (line 16)) (1.4.1)\n",
            "Collecting addict (from basicsr==1.4.2->-r requirements.txt (line 17))\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from basicsr==1.4.2->-r requirements.txt (line 17)) (0.18.3)\n",
            "Collecting lmdb (from basicsr==1.4.2->-r requirements.txt (line 17))\n",
            "  Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from basicsr==1.4.2->-r requirements.txt (line 17)) (2.27.1)\n",
            "Collecting tb-nightly (from basicsr==1.4.2->-r requirements.txt (line 17))\n",
            "  Downloading tb_nightly-2.14.0a20230719-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m124.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from basicsr==1.4.2->-r requirements.txt (line 17)) (0.15.2+cu118)\n",
            "Collecting yapf (from basicsr==1.4.2->-r requirements.txt (line 17))\n",
            "  Downloading yapf-0.40.1-py3-none-any.whl (250 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.3/250.3 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filterpy (from facexlib==0.3.0->-r requirements.txt (line 18))\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 1)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 1)) (16.0.6)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->-r requirements.txt (line 7)) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->-r requirements.txt (line 7)) (67.7.2)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio->-r requirements.txt (line 19))\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: aiohttp~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 19)) (3.8.4)\n",
            "Collecting fastapi (from gradio->-r requirements.txt (line 19))\n",
            "  Downloading fastapi-0.100.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio->-r requirements.txt (line 19))\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client>=0.2.10 (from gradio->-r requirements.txt (line 19))\n",
            "  Downloading gradio_client-0.2.10-py3-none-any.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.0/289.0 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio->-r requirements.txt (line 19))\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.14.0 (from gradio->-r requirements.txt (line 19))\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 19)) (3.0.0)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 19)) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 19)) (3.7.1)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio->-r requirements.txt (line 19))\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson~=3.0 (from gradio->-r requirements.txt (line 19))\n",
            "  Downloading orjson-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 19)) (1.5.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 19)) (1.10.11)\n",
            "Collecting python-multipart (from gradio->-r requirements.txt (line 19))\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio->-r requirements.txt (line 19))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting uvicorn>=0.14.0 (from gradio->-r requirements.txt (line 19))\n",
            "  Downloading uvicorn-0.23.1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<12.0,>=10.0 (from gradio->-r requirements.txt (line 19))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 23)) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 23))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from mdtex2html->-r requirements.txt (line 27)) (3.4.3)\n",
            "Collecting latex2mathml (from mdtex2html->-r requirements.txt (line 27))\n",
            "  Downloading latex2mathml-3.76.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 29)) (5.9.5)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair->-r requirements.txt (line 32)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair->-r requirements.txt (line 32)) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair->-r requirements.txt (line 32)) (0.12.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio->-r requirements.txt (line 19)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio->-r requirements.txt (line 19)) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio->-r requirements.txt (line 19)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio->-r requirements.txt (line 19)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio->-r requirements.txt (line 19)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio->-r requirements.txt (line 19)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio->-r requirements.txt (line 19)) (1.3.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.10->gradio->-r requirements.txt (line 19)) (2023.6.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair->-r requirements.txt (line 32)) (0.19.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio->-r requirements.txt (line 19)) (0.1.2)\n",
            "Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify]>=2.0.0->gradio->-r requirements.txt (line 19))\n",
            "  Downloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 19)) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 19)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 19)) (4.41.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 19)) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 19)) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 19)) (2.8.2)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio->-r requirements.txt (line 19))\n",
            "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
            "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
            "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio->-r requirements.txt (line 19))\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 19)) (2022.7.1)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa==0.9.2->-r requirements.txt (line 6)) (1.4.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->basicsr==1.4.2->-r requirements.txt (line 17)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->basicsr==1.4.2->-r requirements.txt (line 17)) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->basicsr==1.4.2->-r requirements.txt (line 17)) (3.4)\n",
            "INFO: pip is looking at multiple versions of scikit-learn to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting scikit-learn>=0.19.1 (from librosa==0.9.2->-r requirements.txt (line 6))\n",
            "  Downloading scikit_learn-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scikit_learn-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scikit_learn-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scikit_learn-1.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->librosa==0.9.2->-r requirements.txt (line 6)) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.10.2->librosa==0.9.2->-r requirements.txt (line 6)) (1.15.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio->-r requirements.txt (line 19)) (8.1.4)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio->-r requirements.txt (line 19))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio->-r requirements.txt (line 19))\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio->-r requirements.txt (line 19))\n",
            "  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->-r requirements.txt (line 19)) (1.3.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr==1.4.2->-r requirements.txt (line 17)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr==1.4.2->-r requirements.txt (line 17)) (1.56.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr==1.4.2->-r requirements.txt (line 17)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr==1.4.2->-r requirements.txt (line 17)) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr==1.4.2->-r requirements.txt (line 17)) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr==1.4.2->-r requirements.txt (line 17)) (2.3.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr==1.4.2->-r requirements.txt (line 17)) (0.40.0)\n",
            "Collecting importlib-metadata>=6.6.0 (from yapf->basicsr==1.4.2->-r requirements.txt (line 17))\n",
            "  Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->basicsr==1.4.2->-r requirements.txt (line 17)) (3.8.1)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->basicsr==1.4.2->-r requirements.txt (line 17)) (2.0.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.9.2->-r requirements.txt (line 6)) (2.21)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr==1.4.2->-r requirements.txt (line 17)) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr==1.4.2->-r requirements.txt (line 17)) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr==1.4.2->-r requirements.txt (line 17)) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr==1.4.2->-r requirements.txt (line 17)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tb-nightly->basicsr==1.4.2->-r requirements.txt (line 17)) (1.3.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio->-r requirements.txt (line 19)) (3.7.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->basicsr==1.4.2->-r requirements.txt (line 17)) (3.16.1)\n",
            "Collecting uc-micro-py (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio->-r requirements.txt (line 19))\n",
            "  Downloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio->-r requirements.txt (line 19)) (1.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly->basicsr==1.4.2->-r requirements.txt (line 17)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tb-nightly->basicsr==1.4.2->-r requirements.txt (line 17)) (3.2.2)\n",
            "Building wheels for collected packages: basicsr, webrtcvad, ffmpy, filterpy\n",
            "  Building wheel for basicsr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for basicsr: filename=basicsr-1.4.2-py3-none-any.whl size=214821 sha256=a67f1ef33b9514c6e3ae60f4d4eecabe6570b63b09b0512f5f6180665ad61f05\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/83/99/2d8437cc652a01af27df5ff037a4075e95b52d67705c5f30ca\n",
            "  Building wheel for webrtcvad (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for webrtcvad: filename=webrtcvad-2.0.10-cp310-cp310-linux_x86_64.whl size=73469 sha256=43a3ac18421dfc53da813d3ec1aff4669716cc5458dd37a9585fc70a6272af21\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/2b/84/ac7bacfe8c68a87c1ee3dd3c66818a54c71599abf308e8eb35\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=d24c493f36ad944ab8c4547dfb6bb1a11e0f91903872159b9d016b16566ee065\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110459 sha256=d559738fe4738716ddd955a5602dbf65aa436f1ed474e139fb8f78b149547553\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/0c/ea/218f266af4ad626897562199fbbcba521b8497303200186102\n",
            "Successfully built basicsr webrtcvad ffmpy filterpy\n",
            "Installing collected packages: webrtcvad, tokenizers, sentencepiece, safetensors, pydub, lmdb, ffmpy, cpm_kernels, av, addict, yacs, websockets, uc-micro-py, semantic-version, python-multipart, orjson, numpy, markdown-it-py, loguru, latex2mathml, joblib, importlib-metadata, imageio-ffmpeg, h11, aiofiles, yapf, uvicorn, starlette, mdtex2html, mdit-py-plugins, linkify-it-py, imageio, huggingface-hub, httpcore, transformers, scikit-learn, resampy, httpx, fastapi, edge_tts, tb-nightly, librosa, gradio-client, filterpy, gradio, facexlib, basicsr, kornia, gfpgan, face_alignment, accelerate\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.3.1\n",
            "    Uninstalling joblib-1.3.1:\n",
            "      Successfully uninstalled joblib-1.3.1\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.6.4\n",
            "    Uninstalling importlib-metadata-4.6.4:\n",
            "      Successfully uninstalled importlib-metadata-4.6.4\n",
            "  Attempting uninstall: imageio-ffmpeg\n",
            "    Found existing installation: imageio-ffmpeg 0.4.8\n",
            "    Uninstalling imageio-ffmpeg-0.4.8:\n",
            "      Successfully uninstalled imageio-ffmpeg-0.4.8\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.25.1\n",
            "    Uninstalling imageio-2.25.1:\n",
            "      Successfully uninstalled imageio-2.25.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.10.0.post2\n",
            "    Uninstalling librosa-0.10.0.post2:\n",
            "      Successfully uninstalled librosa-0.10.0.post2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imbalanced-learn 0.10.1 requires joblib>=1.1.1, but you have joblib 1.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.21.0 addict-2.4.0 aiofiles-23.1.0 av-10.0.0 basicsr-1.4.2 cpm_kernels-1.0.11 edge_tts-6.1.7 face_alignment-1.3.5 facexlib-0.3.0 fastapi-0.100.0 ffmpy-0.3.1 filterpy-1.4.5 gfpgan-1.3.8 gradio-3.37.0 gradio-client-0.2.10 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 huggingface-hub-0.16.4 imageio-2.19.3 imageio-ffmpeg-0.4.7 importlib-metadata-6.8.0 joblib-1.1.0 kornia-0.6.8 latex2mathml-3.76.0 librosa-0.9.2 linkify-it-py-2.0.2 lmdb-1.4.1 loguru-0.7.0 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 mdtex2html-1.2.0 numpy-1.23.4 orjson-3.9.2 pydub-0.25.1 python-multipart-0.0.6 resampy-0.3.1 safetensors-0.3.1 scikit-learn-1.1.3 semantic-version-2.10.0 sentencepiece-0.1.99 starlette-0.27.0 tb-nightly-2.14.0a20230719 tokenizers-0.13.3 transformers-4.31.0 uc-micro-py-1.0.2 uvicorn-0.23.1 webrtcvad-2.0.10 websockets-11.0.3 yacs-0.1.8 yapf-0.40.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os._exit(00) # restart notebook"
      ],
      "metadata": {
        "id": "uGEMkTz8tEo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd ChatGLM2-SadTalker/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKYBlW0LtB_V",
        "outputId": "4fca9f37-377b-42bc-9b9b-7a607d7078ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ChatGLM2-SadTalker\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import librosa\n",
        "import gradio as gr\n",
        "from scipy.io.wavfile import write\n",
        "from transformers import WavLMModel\n",
        "\n",
        "import utils\n",
        "from models import SynthesizerTrn\n",
        "from mel_processing import mel_spectrogram_torch\n",
        "from speaker_encoder.voice_encoder import SpeakerEncoder\n",
        "\n",
        "import time\n",
        "from textwrap import dedent\n",
        "\n",
        "import mdtex2html\n",
        "from loguru import logger\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "from tts_voice import tts_order_voice\n",
        "import edge_tts\n",
        "import tempfile\n",
        "import anyio\n",
        "\n",
        "import os, sys\n",
        "from src.gradio_demo import SadTalker\n",
        "\n",
        "\n",
        "try:\n",
        "    import webui  # in webui\n",
        "    in_webui = True\n",
        "except:\n",
        "    in_webui = False\n",
        "\n",
        "\n",
        "def toggle_audio_file(choice):\n",
        "    if choice == False:\n",
        "        return gr.update(visible=True), gr.update(visible=False)\n",
        "    else:\n",
        "        return gr.update(visible=False), gr.update(visible=True)\n",
        "\n",
        "def ref_video_fn(path_of_ref_video):\n",
        "    if path_of_ref_video is not None:\n",
        "        return gr.update(value=True)\n",
        "    else:\n",
        "        return gr.update(value=False)\n",
        "\n",
        "sad_talker = SadTalker(\"checkpoints\", \"src/config\", lazy_load=True)\n",
        "\n",
        "'''\n",
        "def get_wavlm():\n",
        "    os.system('gdown https://drive.google.com/uc?id=12-cB34qCTvByWT-QtOcZaqwwO21FLSqU')\n",
        "    shutil.move('WavLM-Large.pt', 'wavlm')\n",
        "'''\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "smodel = SpeakerEncoder('speaker_encoder/ckpt/pretrained_bak_5805000.pt')\n",
        "\n",
        "print(\"Loading FreeVC(24k)...\")\n",
        "hps = utils.get_hparams_from_file(\"configs/freevc-24.json\")\n",
        "freevc_24 = SynthesizerTrn(\n",
        "    hps.data.filter_length // 2 + 1,\n",
        "    hps.train.segment_size // hps.data.hop_length,\n",
        "    **hps.model).to(device)\n",
        "_ = freevc_24.eval()\n",
        "_ = utils.load_checkpoint(\"checkpoint/freevc-24.pth\", freevc_24, None)\n",
        "\n",
        "print(\"Loading WavLM for content...\")\n",
        "cmodel = WavLMModel.from_pretrained(\"microsoft/wavlm-large\").to(device)\n",
        "\n",
        "def convert(model, src, tgt):\n",
        "    with torch.no_grad():\n",
        "        # tgt\n",
        "        wav_tgt, _ = librosa.load(tgt, sr=hps.data.sampling_rate)\n",
        "        wav_tgt, _ = librosa.effects.trim(wav_tgt, top_db=20)\n",
        "        if model == \"FreeVC\" or model == \"FreeVC (24kHz)\":\n",
        "            g_tgt = smodel.embed_utterance(wav_tgt)\n",
        "            g_tgt = torch.from_numpy(g_tgt).unsqueeze(0).to(device)\n",
        "        else:\n",
        "            wav_tgt = torch.from_numpy(wav_tgt).unsqueeze(0).to(device)\n",
        "            mel_tgt = mel_spectrogram_torch(\n",
        "                wav_tgt,\n",
        "                hps.data.filter_length,\n",
        "                hps.data.n_mel_channels,\n",
        "                hps.data.sampling_rate,\n",
        "                hps.data.hop_length,\n",
        "                hps.data.win_length,\n",
        "                hps.data.mel_fmin,\n",
        "                hps.data.mel_fmax\n",
        "            )\n",
        "        # src\n",
        "        wav_src, _ = librosa.load(src, sr=hps.data.sampling_rate)\n",
        "        wav_src = torch.from_numpy(wav_src).unsqueeze(0).to(device)\n",
        "        c = cmodel(wav_src).last_hidden_state.transpose(1, 2).to(device)\n",
        "        # infer\n",
        "        if model == \"FreeVC\":\n",
        "            audio = freevc.infer(c, g=g_tgt)\n",
        "        elif model == \"FreeVC-s\":\n",
        "            audio = freevc_s.infer(c, mel=mel_tgt)\n",
        "        else:\n",
        "            audio = freevc_24.infer(c, g=g_tgt)\n",
        "        audio = audio[0][0].data.cpu().float().numpy()\n",
        "        if model == \"FreeVC\" or model == \"FreeVC-s\":\n",
        "            write(\"out.wav\", hps.data.sampling_rate, audio)\n",
        "        else:\n",
        "            write(\"out.wav\", 24000, audio)\n",
        "    out = \"out.wav\"\n",
        "    return out\n",
        "\n",
        "# GLM2\n",
        "\n",
        "language_dict = tts_order_voice\n",
        "\n",
        "# fix timezone in Linux\n",
        "os.environ[\"TZ\"] = \"Asia/Shanghai\"\n",
        "try:\n",
        "    time.tzset()  # type: ignore # pylint: disable=no-member\n",
        "except Exception:\n",
        "    # Windows\n",
        "    logger.warning(\"Windows, cant run time.tzset()\")\n",
        "\n",
        "# model_name = \"THUDM/chatglm2-6b\"\n",
        "model_name = \"THUDM/chatglm2-6b\"\n",
        "\n",
        "RETRY_FLAG = False\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "# model = AutoModel.from_pretrained(model_name, trust_remote_code=True).cuda()\n",
        "\n",
        "# 4/8 bit\n",
        "# model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True).quantize(4).cuda()\n",
        "\n",
        "has_cuda = torch.cuda.is_available()\n",
        "\n",
        "# has_cuda = False  # force cpu\n",
        "\n",
        "if has_cuda:\n",
        "    model_glm = (\n",
        "        AutoModel.from_pretrained(model_name, trust_remote_code=True).cuda().half()\n",
        "    )  # 3.92G\n",
        "else:\n",
        "    model_glm = AutoModel.from_pretrained(\n",
        "        model_name, trust_remote_code=True\n",
        "    ).float()  # .float() .half().float()\n",
        "\n",
        "model_glm = model_glm.eval()\n",
        "\n",
        "_ = \"\"\"Override Chatbot.postprocess\"\"\"\n",
        "\n",
        "\n",
        "def postprocess(self, y):\n",
        "    if y is None:\n",
        "        return []\n",
        "    for i, (message, response) in enumerate(y):\n",
        "        y[i] = (\n",
        "            None if message is None else mdtex2html.convert((message)),\n",
        "            None if response is None else mdtex2html.convert(response),\n",
        "        )\n",
        "    return y\n",
        "\n",
        "\n",
        "gr.Chatbot.postprocess = postprocess\n",
        "\n",
        "\n",
        "def parse_text(text):\n",
        "    \"\"\"copy from https://github.com/GaiZhenbiao/ChuanhuChatGPT/\"\"\"\n",
        "    lines = text.split(\"\\n\")\n",
        "    lines = [line for line in lines if line != \"\"]\n",
        "    count = 0\n",
        "    for i, line in enumerate(lines):\n",
        "        if \"```\" in line:\n",
        "            count += 1\n",
        "            items = line.split(\"`\")\n",
        "            if count % 2 == 1:\n",
        "                lines[i] = f'<pre><code class=\"language-{items[-1]}\">'\n",
        "            else:\n",
        "                lines[i] = \"<br></code></pre>\"\n",
        "        else:\n",
        "            if i > 0:\n",
        "                if count % 2 == 1:\n",
        "                    line = line.replace(\"`\", r\"\\`\")\n",
        "                    line = line.replace(\"<\", \"&lt;\")\n",
        "                    line = line.replace(\">\", \"&gt;\")\n",
        "                    line = line.replace(\" \", \"&nbsp;\")\n",
        "                    line = line.replace(\"*\", \"&ast;\")\n",
        "                    line = line.replace(\"_\", \"&lowbar;\")\n",
        "                    line = line.replace(\"-\", \"&#45;\")\n",
        "                    line = line.replace(\".\", \"&#46;\")\n",
        "                    line = line.replace(\"!\", \"&#33;\")\n",
        "                    line = line.replace(\"(\", \"&#40;\")\n",
        "                    line = line.replace(\")\", \"&#41;\")\n",
        "                    line = line.replace(\"$\", \"&#36;\")\n",
        "                lines[i] = \"<br>\" + line\n",
        "    text = \"\".join(lines)\n",
        "    return text\n",
        "\n",
        "\n",
        "def predict(\n",
        "    RETRY_FLAG, input, chatbot, max_length, top_p, temperature, history, past_key_values\n",
        "):\n",
        "    try:\n",
        "        chatbot.append((parse_text(input), \"\"))\n",
        "    except Exception as exc:\n",
        "        logger.error(exc)\n",
        "        logger.debug(f\"{chatbot=}\")\n",
        "        _ = \"\"\"\n",
        "        if chatbot:\n",
        "            chatbot[-1] = (parse_text(input), str(exc))\n",
        "            yield chatbot, history, past_key_values\n",
        "        # \"\"\"\n",
        "        yield chatbot, history, past_key_values\n",
        "\n",
        "    for response, history, past_key_values in model_glm.stream_chat(\n",
        "        tokenizer,\n",
        "        input,\n",
        "        history,\n",
        "        past_key_values=past_key_values,\n",
        "        return_past_key_values=True,\n",
        "        max_length=max_length,\n",
        "        top_p=top_p,\n",
        "        temperature=temperature,\n",
        "    ):\n",
        "        chatbot[-1] = (parse_text(input), parse_text(response))\n",
        "        # chatbot[-1][-1] = parse_text(response)\n",
        "\n",
        "        yield chatbot, history, past_key_values, parse_text(response)\n",
        "\n",
        "\n",
        "def trans_api(input, max_length=4096, top_p=0.8, temperature=0.2):\n",
        "    if max_length < 10:\n",
        "        max_length = 4096\n",
        "    if top_p < 0.1 or top_p > 1:\n",
        "        top_p = 0.85\n",
        "    if temperature <= 0 or temperature > 1:\n",
        "        temperature = 0.01\n",
        "    try:\n",
        "        res, _ = model_glm.chat(\n",
        "            tokenizer,\n",
        "            input,\n",
        "            history=[],\n",
        "            past_key_values=None,\n",
        "            max_length=max_length,\n",
        "            top_p=top_p,\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        # logger.debug(f\"{res=} \\n{_=}\")\n",
        "    except Exception as exc:\n",
        "        logger.error(f\"{exc=}\")\n",
        "        res = str(exc)\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "def reset_user_input():\n",
        "    return gr.update(value=\"\")\n",
        "\n",
        "\n",
        "def reset_state():\n",
        "    return [], [], None, \"\"\n",
        "\n",
        "\n",
        "# Delete last turn\n",
        "def delete_last_turn(chat, history):\n",
        "    if chat and history:\n",
        "        chat.pop(-1)\n",
        "        history.pop(-1)\n",
        "    return chat, history\n",
        "\n",
        "\n",
        "# Regenerate response\n",
        "def retry_last_answer(\n",
        "    user_input, chatbot, max_length, top_p, temperature, history, past_key_values\n",
        "):\n",
        "    if chatbot and history:\n",
        "        # Removing the previous conversation from chat\n",
        "        chatbot.pop(-1)\n",
        "        # Setting up a flag to capture a retry\n",
        "        RETRY_FLAG = True\n",
        "        # Getting last message from user\n",
        "        user_input = history[-1][0]\n",
        "        # Removing bot response from the history\n",
        "        history.pop(-1)\n",
        "\n",
        "    yield from predict(\n",
        "        RETRY_FLAG,  # type: ignore\n",
        "        user_input,\n",
        "        chatbot,\n",
        "        max_length,\n",
        "        top_p,\n",
        "        temperature,\n",
        "        history,\n",
        "        past_key_values,\n",
        "    )\n",
        "\n",
        "# print\n",
        "\n",
        "def print(text):\n",
        "    return text\n",
        "\n",
        "# TTS\n",
        "\n",
        "async def text_to_speech_edge(text, language_code):\n",
        "    voice = language_dict[language_code]\n",
        "    communicate = edge_tts.Communicate(text, voice)\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as tmp_file:\n",
        "        tmp_path = tmp_file.name\n",
        "\n",
        "    await communicate.save(tmp_path)\n",
        "\n",
        "    return tmp_path\n",
        "\n",
        "\n",
        "with gr.Blocks(title=\"ChatGLM2-6B-int4\", theme=gr.themes.Soft(text_size=\"sm\")) as demo:\n",
        "    gr.HTML(\"<center>\"\n",
        "            \"<h1>🥳💕🎶 - ChatGLM2 + 声音克隆：和你喜欢的角色畅所欲言吧！</h1>\"\n",
        "            \"</center>\")\n",
        "    gr.Markdown(\"## <center>💡 - 第二代ChatGLM大语言模型 + FreeVC变声，为您打造独一无二的沉浸式对话体验，支持中英双语</center>\")\n",
        "    gr.Markdown(\"## <center>🌊 - 更多精彩应用，尽在[滔滔AI](http://www.talktalkai.com)；滔滔AI，为爱滔滔！💕</center>\")\n",
        "    gr.Markdown(\"### <center>⭐ - 如果您喜欢这个程序，欢迎给我的[Github项目](https://github.com/KevinWang676/ChatGLM2-Voice-Cloning)点赞支持！</center>\")\n",
        "    with gr.Tab(\"🍻 - ChatGLM2聊天区\"):\n",
        "        with gr.Accordion(\"📒 相关信息\", open=False):\n",
        "            _ = f\"\"\" ChatGLM2的可选参数信息：\n",
        "                * Low temperature: responses will be more deterministic and focused; High temperature: responses more creative.\n",
        "                * Suggested temperatures -- translation: up to 0.3; chatting: > 0.4\n",
        "                * Top P controls dynamic vocabulary selection based on context.\\n\n",
        "                如果您想让ChatGLM2进行角色扮演并与之对话，请先输入恰当的提示词，如“请你扮演成动漫角色蜡笔小新并和我进行对话”；您也可以为ChatGLM2提供自定义的角色设定\\n\n",
        "                当您使用声音克隆功能时，请先在此程序的对应位置上传一段您喜欢的音频\n",
        "                \"\"\"\n",
        "            gr.Markdown(dedent(_))\n",
        "        chatbot = gr.Chatbot(height=300)\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=4):\n",
        "                with gr.Column(scale=12):\n",
        "                    user_input = gr.Textbox(\n",
        "                        label=\"请在此处和GLM2聊天 (按回车键即可发送)\",\n",
        "                        placeholder=\"聊点什么吧\",\n",
        "                    )\n",
        "                    RETRY_FLAG = gr.Checkbox(value=False, visible=False)\n",
        "        with gr.Column(min_width=32, scale=1):\n",
        "            with gr.Row():\n",
        "                submitBtn = gr.Button(\"开始和GLM2交流吧\", variant=\"primary\")\n",
        "                deleteBtn = gr.Button(\"删除最新一轮对话\", variant=\"secondary\")\n",
        "                retryBtn = gr.Button(\"重新生成最新一轮对话\", variant=\"secondary\")\n",
        "\n",
        "        with gr.Accordion(\"🔧 更多设置\", open=False):\n",
        "            with gr.Row():\n",
        "                emptyBtn = gr.Button(\"清空所有聊天记录\")\n",
        "                max_length = gr.Slider(\n",
        "                    0,\n",
        "                    32768,\n",
        "                    value=8192,\n",
        "                    step=1.0,\n",
        "                    label=\"Maximum length\",\n",
        "                    interactive=True,\n",
        "                )\n",
        "                top_p = gr.Slider(\n",
        "                    0, 1, value=0.85, step=0.01, label=\"Top P\", interactive=True\n",
        "                )\n",
        "                temperature = gr.Slider(\n",
        "                    0.01, 1, value=0.95, step=0.01, label=\"Temperature\", interactive=True\n",
        "                )\n",
        "\n",
        "\n",
        "        with gr.Row():\n",
        "            test1 = gr.Textbox(label=\"GLM2的最新回答 (可编辑)\", lines = 3)\n",
        "            with gr.Column():\n",
        "                language = gr.Dropdown(choices=list(language_dict.keys()), value=\"普通话 (中国大陆)-Xiaoxiao-女\", label=\"请选择文本对应的语言及您喜欢的说话人\")\n",
        "                tts_btn = gr.Button(\"生成对应的音频吧\", variant=\"primary\")\n",
        "            output_audio = gr.Audio(type=\"filepath\", label=\"为您生成的音频\", interactive=False)\n",
        "\n",
        "        tts_btn.click(text_to_speech_edge, inputs=[test1, language], outputs=[output_audio])\n",
        "\n",
        "        with gr.Row():\n",
        "            model_choice = gr.Dropdown(choices=[\"FreeVC\", \"FreeVC-s\", \"FreeVC (24kHz)\"], value=\"FreeVC (24kHz)\", label=\"Model\", visible=False)\n",
        "            audio1 = output_audio\n",
        "            audio2 = gr.Audio(label=\"请上传您喜欢的声音进行声音克隆\", type='filepath')\n",
        "            clone_btn = gr.Button(\"开始AI声音克隆吧\", variant=\"primary\")\n",
        "            audio_cloned =  gr.Audio(label=\"为您生成的专属声音克隆音频\", type='filepath')\n",
        "\n",
        "        clone_btn.click(convert, inputs=[model_choice, audio1, audio2], outputs=[audio_cloned])\n",
        "\n",
        "        history = gr.State([])\n",
        "        past_key_values = gr.State(None)\n",
        "\n",
        "        user_input.submit(\n",
        "            predict,\n",
        "            [\n",
        "                RETRY_FLAG,\n",
        "                user_input,\n",
        "                chatbot,\n",
        "                max_length,\n",
        "                top_p,\n",
        "                temperature,\n",
        "                history,\n",
        "                past_key_values,\n",
        "            ],\n",
        "            [chatbot, history, past_key_values, test1],\n",
        "            show_progress=\"full\",\n",
        "        )\n",
        "        submitBtn.click(\n",
        "            predict,\n",
        "            [\n",
        "                RETRY_FLAG,\n",
        "                user_input,\n",
        "                chatbot,\n",
        "                max_length,\n",
        "                top_p,\n",
        "                temperature,\n",
        "                history,\n",
        "                past_key_values,\n",
        "            ],\n",
        "            [chatbot, history, past_key_values, test1],\n",
        "            show_progress=\"full\",\n",
        "            api_name=\"predict\",\n",
        "        )\n",
        "        submitBtn.click(reset_user_input, [], [user_input])\n",
        "\n",
        "        emptyBtn.click(\n",
        "            reset_state, outputs=[chatbot, history, past_key_values, test1], show_progress=\"full\"\n",
        "        )\n",
        "\n",
        "        retryBtn.click(\n",
        "            retry_last_answer,\n",
        "            inputs=[\n",
        "                user_input,\n",
        "                chatbot,\n",
        "                max_length,\n",
        "                top_p,\n",
        "                temperature,\n",
        "                history,\n",
        "                past_key_values,\n",
        "            ],\n",
        "            # outputs = [chatbot, history, last_user_message, user_message]\n",
        "            outputs=[chatbot, history, past_key_values, test1],\n",
        "        )\n",
        "        deleteBtn.click(delete_last_turn, [chatbot, history], [chatbot, history])\n",
        "\n",
        "        with gr.Accordion(\"📔 提示词示例\", open=False):\n",
        "            etext = \"\"\"In America, where cars are an important part of the national psyche, a decade ago people had suddenly started to drive less, which had not happened since the oil shocks of the 1970s. \"\"\"\n",
        "            examples = gr.Examples(\n",
        "                examples=[\n",
        "                    [\"Explain the plot of Cinderella in a sentence.\"],\n",
        "                    [\n",
        "                        \"How long does it take to become proficient in French, and what are the best methods for retaining information?\"\n",
        "                    ],\n",
        "                    [\"What are some common mistakes to avoid when writing code?\"],\n",
        "                    [\"Build a prompt to generate a beautiful portrait of a horse\"],\n",
        "                    [\"Suggest four metaphors to describe the benefits of AI\"],\n",
        "                    [\"Write a pop song about leaving home for the sandy beaches.\"],\n",
        "                    [\"Write a summary demonstrating my ability to tame lions\"],\n",
        "                    [\"鲁迅和周树人什么关系\"],\n",
        "                    [\"从前有一头牛，这头牛后面有什么？\"],\n",
        "                    [\"正无穷大加一大于正无穷大吗？\"],\n",
        "                    [\"正无穷大加正无穷大大于正无穷大吗？\"],\n",
        "                    [\"-2的平方根等于什么\"],\n",
        "                    [\"树上有5只鸟，猎人开枪打死了一只。树上还有几只鸟？\"],\n",
        "                    [\"树上有11只鸟，猎人开枪打死了一只。树上还有几只鸟？提示：需考虑鸟可能受惊吓飞走。\"],\n",
        "                    [\"鲁迅和周树人什么关系 用英文回答\"],\n",
        "                    [\"以红楼梦的行文风格写一张委婉的请假条。不少于320字。\"],\n",
        "                    [f\"{etext} 翻成中文，列出3个版本\"],\n",
        "                    [f\"{etext} \\n 翻成中文，保留原意，但使用文学性的语言。不要写解释。列出3个版本\"],\n",
        "                    [\"js 判断一个数是不是质数\"],\n",
        "                    [\"js 实现python 的 range(10)\"],\n",
        "                    [\"js 实现python 的 [*(range(10)]\"],\n",
        "                    [\"假定 1 + 2 = 4, 试求 7 + 8\"],\n",
        "                    [\"Erkläre die Handlung von Cinderella in einem Satz.\"],\n",
        "                    [\"Erkläre die Handlung von Cinderella in einem Satz. Auf Deutsch\"],\n",
        "                ],\n",
        "                inputs=[user_input],\n",
        "                examples_per_page=30,\n",
        "            )\n",
        "\n",
        "        with gr.Accordion(\"For Chat/Translation API\", open=False, visible=False):\n",
        "            input_text = gr.Text()\n",
        "            tr_btn = gr.Button(\"Go\", variant=\"primary\")\n",
        "            out_text = gr.Text()\n",
        "        tr_btn.click(\n",
        "            trans_api,\n",
        "            [input_text, max_length, top_p, temperature],\n",
        "            out_text,\n",
        "            # show_progress=\"full\",\n",
        "            api_name=\"tr\",\n",
        "        )\n",
        "        _ = \"\"\"\n",
        "        input_text.submit(\n",
        "            trans_api,\n",
        "            [input_text, max_length, top_p, temperature],\n",
        "            out_text,\n",
        "            show_progress=\"full\",\n",
        "            api_name=\"tr1\",\n",
        "        )\n",
        "        # \"\"\"\n",
        "\n",
        "    with gr.Tab(\"📺 - 视频聊天区\"):\n",
        "        with gr.Row().style(equal_height=False):\n",
        "            with gr.Column(variant='panel'):\n",
        "                with gr.Tabs(elem_id=\"sadtalker_source_image\"):\n",
        "                    with gr.TabItem('图片上传'):\n",
        "                        with gr.Row():\n",
        "                            source_image = gr.Image(label=\"请上传一张您喜欢角色的图片\", source=\"upload\", type=\"filepath\", elem_id=\"img2img_image\").style(width=512)\n",
        "\n",
        "                with gr.Tabs(elem_id=\"sadtalker_driven_audio\"):\n",
        "                    with gr.TabItem('💡您还可以将视频下载到本地'):\n",
        "                        with gr.Column(variant='panel'):\n",
        "                            driven_audio = audio_cloned\n",
        "            with gr.Column(variant='panel'):\n",
        "                with gr.Tabs(elem_id=\"sadtalker_checkbox\"):\n",
        "                    with gr.TabItem('视频设置'):\n",
        "                        gr.Markdown(\"need help? please visit our [best practice page](https://github.com/OpenTalker/SadTalker/blob/main/docs/best_practice.md) for more detials\")\n",
        "                        with gr.Column(variant='panel'):\n",
        "                            # width = gr.Slider(minimum=64, elem_id=\"img2img_width\", maximum=2048, step=8, label=\"Manually Crop Width\", value=512) # img2img_width\n",
        "                            # height = gr.Slider(minimum=64, elem_id=\"img2img_height\", maximum=2048, step=8, label=\"Manually Crop Height\", value=512) # img2img_width\n",
        "                            pose_style = gr.Slider(minimum=0, maximum=46, step=1, label=\"Pose style\", value=0, visible=False)\n",
        "                            size_of_image = gr.Radio([256, 512], value=256, label='face model resolution', info=\"use 256/512 model?\", visible=False)\n",
        "                            preprocess_type = gr.Radio(['crop', 'full'], value='crop', label='是否聚焦角色面部', info=\"crop：视频会聚焦角色面部；full：视频会显示图片全貌\")\n",
        "                            is_still_mode = gr.Checkbox(label=\"静态模式 (开启静态模式，角色的面部动作会减少；默认开启)\", value=True)\n",
        "                            batch_size = gr.Slider(label=\"Batch size (数值越大，生成速度越快；若显卡性能好，可增大数值)\", step=1, maximum=8, value=1)\n",
        "                            enhancer = gr.Checkbox(label=\"GFPGAN as Face enhancer\", value=False, visible=False)\n",
        "                            submit = gr.Button('开始视频聊天吧', elem_id=\"sadtalker_generate\", variant='primary')\n",
        "\n",
        "                with gr.Tabs(elem_id=\"sadtalker_genearted\"):\n",
        "                        gen_video = gr.Video(label=\"Generated video\", format=\"mp4\").style(width=256)\n",
        "\n",
        "        submit.click(\n",
        "                    fn=sad_talker.test,\n",
        "                    inputs=[source_image,\n",
        "                            driven_audio,\n",
        "                            preprocess_type,\n",
        "                            is_still_mode,\n",
        "                            enhancer,\n",
        "                            batch_size,\n",
        "                            size_of_image,\n",
        "                            pose_style\n",
        "                            ],\n",
        "                    outputs=[gen_video]\n",
        "                    )\n",
        "\n",
        "\n",
        "    gr.Markdown(\"### <center>注意❗：请不要生成会对个人以及组织造成侵害的内容，此程序仅供科研、学习及个人娱乐使用。</center>\")\n",
        "    gr.Markdown(\"<center>💡 - 如何使用此程序：输入您对ChatGLM的提问后，依次点击“开始和GLM2交流吧”、“生成对应的音频吧”、“开始AI声音克隆吧”三个按键即可；使用声音克隆功能时，请先上传一段您喜欢的音频</center>\")\n",
        "    gr.HTML('''\n",
        "        <div class=\"footer\">\n",
        "                    <p>🌊🏞️🎶 - 江水东流急，滔滔无尽声。 明·顾璘\n",
        "                    </p>\n",
        "        </div>\n",
        "    ''')\n",
        "\n",
        "\n",
        "demo.queue().launch(show_error=True, debug=True, share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "19b7c00964be485cb1b0412de7ee9a6d",
            "074f822c410e4f08b59531aea5f1c9d2",
            "6b26a437e85649d98fff8e0cf802a40a",
            "334fd68df6e044af9ba12c4906ac22d8",
            "ed2b52119886472790022d3896f5bb91",
            "dc1462b58eef4baca8a478f3b42c4a5b",
            "9785e67ee8504c08b6e67e24faa371c9",
            "cfcc55357c734f729fa58df938dcfaad",
            "dfec84c004504e60bf445065a7a56c1b",
            "6ee82215b11d49fd9aab8942ee7eb76b",
            "2c66c9f792c945de921e4c00b5ea2841"
          ]
        },
        "id": "W0F6POR0snMJ",
        "outputId": "45b6299b-5ece-47b3-9854-a625955f71bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded the voice encoder model on cuda in 0.01 seconds.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19b7c00964be485cb1b0412de7ee9a6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://66d2c205d2846b206c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://66d2c205d2846b206c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using safetensor as default\n",
            "{'checkpoint': 'checkpoints/SadTalker_V0.0.2_256.safetensors', 'dir_of_BFM_fitting': 'src/config', 'audio2pose_yaml_path': 'src/config/auido2pose.yaml', 'audio2exp_yaml_path': 'src/config/auido2exp.yaml', 'use_safetensor': True, 'mappingnet_checkpoint': 'checkpoints/mapping_00229-model.pth.tar', 'facerender_yaml': 'src/config/facerender.yaml'}\n",
            "/tmp/gradio/280d4cb5a66b90e38c0db26ccfe84fd381ddd90d/image.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "landmark Det:: 100%|██████████| 1/1 [00:00<00:00, 21.01it/s]\n",
            "3DMM Extraction In Video:: 100%|██████████| 1/1 [00:00<00:00, 89.45it/s]\n",
            "mel:: 100%|██████████| 207/207 [00:00<00:00, 35650.03it/s]\n",
            "audio2exp:: 100%|██████████| 21/21 [00:00<00:00, 200.52it/s]\n",
            "Face Renderer:: 100%|██████████| 104/104 [00:05<00:00, 18.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The generated video is named ./results/ab44ab5d-c9fe-44e1-a400-4a39323a39b7/image##out-0-100.mp4\n",
            "face enhancer....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Face Enhancer:: 100%|██████████| 207/207 [00:31<00:00,  6.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The generated video is named ./results/ab44ab5d-c9fe-44e1-a400-4a39323a39b7/image##out-0-100_enhanced.mp4\n",
            "The generated video is named image##out-0-100 in ./results/ab44ab5d-c9fe-44e1-a400-4a39323a39b7\n",
            "using safetensor as default\n",
            "{'checkpoint': 'checkpoints/SadTalker_V0.0.2_256.safetensors', 'dir_of_BFM_fitting': 'src/config', 'audio2pose_yaml_path': 'src/config/auido2pose.yaml', 'audio2exp_yaml_path': 'src/config/auido2exp.yaml', 'use_safetensor': True, 'mappingnet_checkpoint': 'checkpoints/mapping_00229-model.pth.tar', 'facerender_yaml': 'src/config/facerender.yaml'}\n",
            "/tmp/gradio/280d4cb5a66b90e38c0db26ccfe84fd381ddd90d/image.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "landmark Det:: 100%|██████████| 1/1 [00:00<00:00, 20.68it/s]\n",
            "3DMM Extraction In Video:: 100%|██████████| 1/1 [00:00<00:00, 87.31it/s]\n",
            "mel:: 100%|██████████| 207/207 [00:00<00:00, 32768.00it/s]\n",
            "audio2exp:: 100%|██████████| 21/21 [00:00<00:00, 369.41it/s]\n",
            "Face Renderer:: 100%|██████████| 13/13 [00:04<00:00,  3.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The generated video is named ./results/e0af8c30-6438-4fc6-b89c-f489956b17eb/image##out-0-100.mp4\n",
            "face enhancer....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Face Enhancer:: 100%|██████████| 207/207 [00:31<00:00,  6.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The generated video is named ./results/e0af8c30-6438-4fc6-b89c-f489956b17eb/image##out-0-100_enhanced.mp4\n",
            "The generated video is named image##out-0-100 in ./results/e0af8c30-6438-4fc6-b89c-f489956b17eb\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://66d2c205d2846b206c.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aZDqf_WPs8Yt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}